## ğŸ”¹ What is **Uvicorn**?

<img width="220" height="720" alt="image" src="https://github.com/user-attachments/assets/5e79dca4-d18b-45eb-896e-927e955fe826" />

**Uvicorn** is a **lightweight, fast ASGI (Asynchronous Server Gateway Interface) web server** written in Python.

* Itâ€™s commonly used to **run FastAPI, Django (with channels), or Starlette apps**.
* Think of it as the engine ğŸï¸ that takes your Python web app and makes it accessible over the web (via HTTP).

---

## ğŸ”¹ Why Uvicorn?

* âš¡ **High performance** â†’ Built on `uvloop` and `httptools` (super-fast libraries).
* ğŸ”„ **Async support** â†’ Handles thousands of requests concurrently (great for APIs).
* ğŸ”Œ **Production-ready** â†’ Often used behind a reverse proxy like **Nginx** or in Docker.

---

## ğŸ”¹ How it works

Normally, if you write a Python API (say with FastAPI), you need a server to handle incoming requests.
Thatâ€™s where **Uvicorn** comes in.

Example (FastAPI app `app.py`):

```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Hello, World!"}
```

Run with **Uvicorn**:

```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

Explanation:

* `app:app` â†’ `filename:FastAPI_object`
* `--host 0.0.0.0` â†’ Accept requests from any IP
* `--port 8000` â†’ Run on port 8000

Now open ğŸ‘‰ `http://localhost:8000`

---

## ğŸ”¹ Real-life Use Cases

* Deploying **FastAPI/Starlette/Django** apps.
* Serving **ML models** as REST APIs.
* Running microservices in Docker & Kubernetes.
